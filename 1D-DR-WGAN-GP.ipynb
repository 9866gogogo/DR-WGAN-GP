{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8df33-c40b-4c34-a420-edd4153df87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.stats import pearsonr\n",
    "# import pywt  # Continuous Wavelet Transform\n",
    "import copy\n",
    "import scipy.stats as st\n",
    "from scipy.special import comb\n",
    "import seaborn as sns\n",
    "from sympy import *\n",
    "import math\n",
    "\n",
    "import kennard_stone as ks \n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c205334-d50d-4695-a879-c3ae907df1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7af4e0-3913-4edd-869f-83abbadce4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../Datas/Paper_data/土壤有机质数据/2024第二批数据(96个土样)/re_vis-NIR.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f4b52-8a85-4c85-b48c-fdcaaa742776",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c73400-9d6a-40ac-ac21-eca24fd7c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,\"X400\":\"X2400\"].values.astype(\"float32\")\n",
    "Y = data[\"SOM\"].values.astype(\"float32\")\n",
    "wavelengths = np.linspace(400, 2400, X.shape[1])\n",
    "train_data = data.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c845f6-f004-4f9e-9866-b94582a00f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hyperspectral_image(_data, title=None, x_label_start=0, sample_interval=10):\n",
    "    y = _data\n",
    "    x = range(0, _data.shape[1])\n",
    "    axis_x_label = range(x_label_start, y.shape[1] * sample_interval + x_label_start, sample_interval)\n",
    "    fig, ax = plt.subplots(figsize=[6, 4],dpi=400)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    for i in range(0, y.shape[0]):\n",
    "        plt.plot(x, y[i])\n",
    "    xticks_interval = 20  \n",
    "    # xticks_interval = 200  \n",
    "    plt.xticks(x[::xticks_interval], axis_x_label[::xticks_interval], rotation=0)\n",
    "    plt.xlabel('Wavelength/nm', fontsize=13)\n",
    "    plt.ylabel('Reflectance', fontsize=13)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.grid(linestyle = '--',alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def SG(data, w=11, p=2):\n",
    "    return signal.savgol_filter(data, w, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba71bd-d918-45a9-a284-b25719546f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hyperspectral_image(SG(X),'Raw',400,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e112cdb4-8f93-4bf2-bfa1-c053309a5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = ks.train_test_split(train_data[:,1:], train_data[:,:1], test_size=0.3)\n",
    "\n",
    "gan_train_data = np.hstack((y_train,x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041dcb7-a41b-4220-be87-54546222a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "normalized_data = scaler.fit_transform(gan_train_data)\n",
    "\n",
    "# reconstructed_data = scaler.inverse_transform(normalized_data)\n",
    "\n",
    "gan_data_matrix = np.zeros((len(normalized_data),217,1)).astype(np.float32)\n",
    "for i in range(len(normalized_data)):\n",
    "    gan_data_matrix[i] = normalized_data[i].reshape((217,1))\n",
    "gan_data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293434fa-f93c-4d29-bab7-c193a9fa0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100      # 噪声向量维度\n",
    "critic_steps = 5      # 判别器训练次数/生成器1次\n",
    "gp_weight = 10       # 梯度惩罚系数\n",
    "signal_length = 217   # 信号长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee558e96-b817-4621-be0a-9a0ab37446fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(signal_length, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "g_model = build_generator()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74099d1-890a-4104-90c4-dfc5fb90f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(signal_length, 1)), \n",
    "        layers.Conv1D(32, 5, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Conv1D(64, 5, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1) \n",
    "    ])\n",
    "    return model\n",
    "d_model = build_critic()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e3cfa-d7c8-43b8-89c1-3a34d064d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(batch_size, real_samples, fake_samples, critic):\n",
    "    alpha = tf.random.normal([batch_size, 1, 1], 0.0, 1.0)\n",
    "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = critic(interpolated)\n",
    "    \n",
    "    gradients = tape.gradient(pred, interpolated)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1,2]))\n",
    "    gp = tf.reduce_mean((norm - 1.0)**2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8fab1-8334-45be-b0fb-62491a14c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GP(keras.Model):\n",
    "    def __init__(self, generator, critic, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "        self.gp_weight = gp_weight\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002, decay_steps=20000, decay_rate=0.9, staircase=True), beta_1=0.5, beta_2=0.999)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002, decay_steps=20000, decay_rate=0.9, staircase=True), beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.critic_loss_tracker = tf.keras.metrics.Mean(name=\"c_loss\")\n",
    "        self.generator_loss_tracker = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.critic_loss_tracker, self.generator_loss_tracker]\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        \n",
    "        for _ in range(critic_steps):\n",
    "            noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_data = self.generator(noise)\n",
    "                fake_data = tf.reshape(fake_data, [-1, signal_length, 1])\n",
    "                \n",
    "                real_output = self.critic(real_data)\n",
    "                fake_output = self.critic(fake_data)\n",
    "\n",
    "                mse_loss = tf.reduce_mean(tf.square(real_data - fake_data))\n",
    "\n",
    "                critic_loss = tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "                \n",
    "                gp = gradient_penalty(batch_size, real_data, fake_data, self.critic)\n",
    "                total_critic_loss = critic_loss + gp * self.gp_weight + mse_loss* self.gp_weight*0.5\n",
    "                \n",
    "            gradients = tape.gradient(total_critic_loss, self.critic.trainable_variables)\n",
    "            self.critic_optimizer.apply_gradients(zip(gradients, self.critic.trainable_variables))\n",
    "        \n",
    "        noise = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated = self.generator(noise)\n",
    "            generated = tf.reshape(generated, [-1, signal_length, 1])\n",
    "            gen_output = self.critic(generated)    \n",
    "            generator_loss = -tf.reduce_mean(gen_output)\n",
    "            \n",
    "        gradients = tape.gradient(generator_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        self.critic_loss_tracker.update_state(total_critic_loss)\n",
    "        self.generator_loss_tracker.update_state(generator_loss)\n",
    "        \n",
    "        return {\n",
    "            \"critic_loss\": self.critic_loss_tracker.result(),\n",
    "            \"generator_loss\": self.generator_loss_tracker.result()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb297999-1d85-4407-83a4-58603adac7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=65, latent_dim=128, last_end_epochs=0):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.last_end_epochs = last_end_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "      if (self.last_end_epochs+epoch+1) % 100 == 0:\n",
    "        self.model.critic.save(f'../../models/1D-AWGAN-GP[20250131]/GAN-D[{self.last_end_epochs+epoch+1}].h5')\n",
    "        self.model.generator.save(f'../../models/1D-AWGAN-GP[20250131]/GAN-G[{self.last_end_epochs+epoch+1}].h5')\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_data = self.model.generator(random_latent_vectors, training=False)\n",
    "        generated_data = generated_data.numpy()\n",
    "        gan_data_matrix = np.zeros((len(generated_data),217)).astype(np.float32)\n",
    "        for i in range(len(gan_data_matrix)):\n",
    "            gan_data_matrix[i] = generated_data[i].reshape((217))\n",
    "            gan_data_matrix[i] = scaler.inverse_transform(gan_data_matrix[i].reshape(1, -1))\n",
    "        reflact = gan_data_matrix[:,1:]\n",
    "        # fig, ax = plt.subplots(figsize=[6, 4],dpi=400)\n",
    "        x = range(350, reflact.shape[1]+350)\n",
    "        for i in range(0, reflact.shape[0]):\n",
    "          plt.plot(x, reflact[i])\n",
    "        plt.grid(linestyle = '--',alpha=0.7)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebb5ca-55ab-4348-81ff-6e9d887f60a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    epochs = 20000\n",
    "    last_end_epochs = 0 \n",
    "    cbk = GANMonitor(num_img=65, latent_dim=latent_dim, last_end_epochs=last_end_epochs)\n",
    "\n",
    "    generator = build_generator()\n",
    "    critic = build_critic()\n",
    "    if last_end_epochs != 0:\n",
    "        generator = tf.keras.models.load_model(f'../../models/1D-AWGAN-GP[20250131]/GAN-G[{last_end_epochs}].h5')\n",
    "        critic = tf.keras.models.load_model(f'../../models/1D-AWGAN-GP[20250131]/GAN-D[{last_end_epochs}].h5')\n",
    "    \n",
    "    wgan_gp = WGAN_GP(generator=generator, critic=critic)\n",
    "    wgan_gp.compile()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(gan_data_matrix)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(65, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    real_signals = dataset\n",
    "    \n",
    "    history = wgan_gp.fit(\n",
    "        real_signals,\n",
    "        batch_size=65,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[cbk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239df69-873b-43c5-83bb-ab2780a91ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799a257-21e6-418c-9471-26c4a9ed14b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f267d63-149a-46e4-a94f-2ee0424b74af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3b8c-5199-4d69-9c21-7fede1eb9e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8b6e2-b8ea-4cfe-8402-adb62f2b66c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e733e9-5e8a-420f-b011-b8a9090afcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
