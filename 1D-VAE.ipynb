{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f5fe62-cee9-4175-bb06-cfe6400a6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.stats import pearsonr\n",
    "# import pywt  # Continuous Wavelet Transform\n",
    "import copy\n",
    "import scipy.stats as st\n",
    "from scipy.special import comb\n",
    "import seaborn as sns\n",
    "from sympy import *\n",
    "import math\n",
    "from tensorflow.keras import layers, Model\n",
    "import kennard_stone as ks \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54192ba5-4535-4dfb-8454-c5c7d4ee7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85463280-81aa-4de3-bc55-5c26146436f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../Datas/Paper_data/土壤有机质数据/2024第二批数据(96个土样)/re_vis-NIR.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cd8c3-586f-4675-9bf3-c208713ce339",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fd0ed-9248-468f-ade9-cd172c4a68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,\"X400\":\"X2400\"].values.astype(\"float32\")\n",
    "Y = data[\"SOM\"].values.astype(\"float32\")\n",
    "wavelengths = np.linspace(400, 2400, X.shape[1])\n",
    "train_data = data.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3064f17-4472-485a-a356-b62f3d62c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hyperspectral_image(_data, title=None, x_label_start=0, sample_interval=10):\n",
    "    y = _data\n",
    "    x = range(0, _data.shape[1])\n",
    "    axis_x_label = range(x_label_start, y.shape[1] * sample_interval + x_label_start, sample_interval)\n",
    "    fig, ax = plt.subplots(figsize=[6, 4],dpi=400)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    for i in range(0, y.shape[0]):\n",
    "        plt.plot(x, y[i])\n",
    "    xticks_interval = 20 \n",
    "    # xticks_interval = 200 \n",
    "    plt.xticks(x[::xticks_interval], axis_x_label[::xticks_interval], rotation=0)\n",
    "    plt.xlabel('Wavelength/nm', fontsize=13)\n",
    "    plt.ylabel('Reflectance', fontsize=13)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.grid(linestyle = '--',alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "def SG(data, w=11, p=2):\n",
    "    return signal.savgol_filter(data, w, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3a8fe-ee9a-4d64-ad96-22f36b8790ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hyperspectral_image(SG(X,w=17,p=2),'Raw',400,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13321b2-b150-4968-a96c-e053b0cea1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = ks.train_test_split(train_data[:,1:], train_data[:,:1], test_size=0.3)\n",
    "\n",
    "gan_train_data = np.hstack((y_train,x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae5492-11fa-40d9-87e3-4b731e5d2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "normalized_data = scaler.fit_transform(gan_train_data)\n",
    "\n",
    "# reconstructed_data = scaler.inverse_transform(normalized_data)\n",
    "\n",
    "gan_data_matrix = np.zeros((len(normalized_data),217,1)).astype(np.float32)\n",
    "for i in range(len(normalized_data)):\n",
    "    gan_data_matrix[i] = normalized_data[i].reshape((217,1))\n",
    "gan_data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc514b4-55a4-4962-ac1d-d0d8d48a71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100      # 噪声向量维度\n",
    "signal_length = 217   # 信号长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9ccf6-80f4-4a39-920a-1ee7e54f8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "    inputs = layers.Input(shape=(signal_length, 1))\n",
    "    x = layers.Conv1D(32, 5, strides=2, padding='same')(inputs)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, 5, strides=2, padding='same')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "    \n",
    "    return Model(inputs, [z_mean, z_log_var], name=\"Encoder\")\n",
    "e_model = build_encoder()\n",
    "e_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb6c9c-2db2-42e2-b626-911c8749cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder():\n",
    "    inputs = layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(32, activation='relu', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    outputs = layers.Dense(signal_length, activation='tanh')(x)\n",
    "    \n",
    "    return Model(inputs, outputs, name=\"Decoder\")\n",
    "\n",
    "d_model = build_decoder()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444f3a2-7ba2-4724-8322-ffddef2aa4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = build_encoder()\n",
    "        self.decoder = build_decoder()\n",
    "        self.kl_weight = 0.01  # KL散度权重系数\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002, decay_steps=10000, decay_rate=0.9, staircase=True), beta_1=0.5, beta_2=0.999)\n",
    "        \n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.rec_loss_tracker = tf.keras.metrics.Mean(name=\"recon_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.rec_loss_tracker,\n",
    "            self.kl_loss_tracker\n",
    "        ]\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = self.reparameterize(z_mean, z_log_var)\n",
    "            \n",
    "            reconstructions = self.decoder(z)\n",
    "            reconstructions = tf.reshape(reconstructions, [-1, signal_length, 1])\n",
    "            \n",
    "            # # 重构损失（L1 + L2）\n",
    "            # rec_loss = tf.reduce_mean(\n",
    "            #     tf.abs(data - reconstructions) + \n",
    "            #     0.5 * tf.square(data - reconstructions)\n",
    "            # )\n",
    "            rec_loss = tf.reduce_mean(\n",
    "                tf.square(data - reconstructions)  # MSE loss\n",
    "            )\n",
    "            \n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "            )\n",
    "            \n",
    "            total_loss = rec_loss + self.kl_weight * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.rec_loss_tracker.update_state(rec_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"recon_loss\": self.rec_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "    }\n",
    "\n",
    "    def generate(self, num_samples):\n",
    "        noise = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "        return self.decoder(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e2276-c9a3-4b46-9720-2cb1d5a818f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=65, latent_dim=128, last_end_epochs=0):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.last_end_epochs = last_end_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "      if (self.last_end_epochs+epoch+1) % 100 == 0:\n",
    "        self.model.encoder.save(f'../../models/1D-VAE[20250131]/GAN-D[{self.last_end_epochs+epoch+1}].h5')\n",
    "        self.model.decoder.save(f'../../models/1D-VAE[20250131]/GAN-G[{self.last_end_epochs+epoch+1}].h5')\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_data = self.model.decoder(random_latent_vectors, training=False)\n",
    "        generated_data = generated_data.numpy()\n",
    "        gan_data_matrix = np.zeros((len(generated_data),217)).astype(np.float32)\n",
    "        for i in range(len(gan_data_matrix)):\n",
    "            gan_data_matrix[i] = generated_data[i].reshape((217))\n",
    "            gan_data_matrix[i] = scaler.inverse_transform(gan_data_matrix[i].reshape(1, -1))\n",
    "        reflact = gan_data_matrix[:,1:]\n",
    "        # fig, ax = plt.subplots(figsize=[6, 4],dpi=400)\n",
    "        x = range(350, reflact.shape[1]+350)\n",
    "        for i in range(0, reflact.shape[0]):\n",
    "          plt.plot(x, reflact[i])\n",
    "        plt.grid(linestyle = '--',alpha=0.7)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b7938-74ed-4bf9-a6eb-f96546262a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    epochs = 20000\n",
    "    last_end_epochs = 0 \n",
    "    cbk = GANMonitor(num_img=65, latent_dim=latent_dim, last_end_epochs=last_end_epochs)\n",
    "    \n",
    "    vae = VAE()\n",
    "    vae.compile()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(gan_data_matrix)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(65, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    real_data = dataset\n",
    "    \n",
    "    history = vae.fit(\n",
    "        real_data,\n",
    "        batch_size=65,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[cbk]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a66ae-a727-49ab-a67f-68f9c072d120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
